最近计划学习下吴恩达在Coursera的机器学习课程视频，发现访问速度奇慢无比，幸好有有心人把课程的讲义都翻译汇总成汉语文字版([详见这里](http://www.ai-start.com/)，其[github项目有视频的下载链接](https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes))，感到方便了不少，感谢他们的工作。学习过程中，我感到有必要把课程概要，以及自己的想法、体会记录下来，作为自己的知识积累。本文是本系列的第一篇，对应课程第一周的内容。

### 提要
第一周介绍了机器学习的相关概念及应用，特别是监督学习和无监督学习的差别；之后以一个实际的例子介绍了线性回归模型，包含了模型的直观理解和求解方法，还包括一些基础线性代数知识--主要是矩阵的相关操作。

### 课程内容
抱歉，我不知道如何给机器学习下定义，如果你有点强迫症，非要一个定义不可，可以问问Tom Mitchell，他是有一个很押韵的定义的。直观上讲，机器学习就是一段程序，这个程序可以解决特定的问题(Task)，在指定一个评价标准（Performance，一般用损失函数度量）并喂给该程序足够多的数据（Experience）之后，它就有了智能，可以预测未知或者输出某些观点。机器学习的应用已经比较广泛了，如google的搜索，医院对某些疾病的诊断，各种图片识别以及最近比较火的语音和文字识别。

监督学习（Supervised Learning）指的是我们喂给程序的数据是标注好的，我们知道每一条数据对应的正确答案是什么，分类和回归都属于监督学习。比如我们有针对某个话题的一批微博评论，已经被**机器学习从业人员**标注成了支持、中性、反对三类，现在来了一条新的评论，机器可以通过提取一些特征把这条新评论归类为上述三类中的某一类。对于无监督学习（Unsupervised Learning）的数据，我们也不知道正确答案是什么，需要机器自己去发现合理的pattern，聚类就属于这种。还是比如我们收集到了某话题的一批微博评论，但是勤劳的**机器学习从业人员**休假了，只能依靠机器自动把数据分成了四堆，事后我们可以查看这四堆堆数据的特征，认为他们分别是支持，中性，反对以及不知道在说啥。由于缺少了一个重要的角色，对于机器而言无监督学习更难一点。

既然是个程序，总少不了敲代码。既然要敲代码，就得纠结用那种语言敲代码。吴恩达推荐Octave/Matlab，不过我认为Python更合适一点。毕业之后，除了画图以外，我已经用Python全面替代了Matlab。本文的例子会涉及一些代码，可以在文末的github地址找到。

线性回归是一种比较简单的机器学习/统计方法，属于监督学习的一种。接下来，我会用一个虚构的例子说明它是如何建模以及求解的。

小明最近需要买房，他想要找出房价$y$和房子大小$x_1$，房间楼层$x_2$的某种关系，从而理性决策。为此，他收集了101条数据，如下表所示：

|房价$y$|大小$x_1$|楼层$x_2$|
|---|---|---|
|91.49| 50|9|
|91.28| 51|16|
|93.83| 52|9|
|...| ...|...|
|247.43|149|7|
|250.34|150|5|

画出来，$y$和$x_1$的关系大概如图1所示, 可以看出是有线性关系的。
![图1](https://mmbiz.qpic.cn/mmbiz_jpg/5Wj1VCYtIxOd9ozic3q56ClpNPFBIQiaRjfKib1Kgsu9JEyPxcMQ9T5KndIWO9ZoIjZzmVNwGrdDWAxGaI8lfFRhQ/0?wx_fmt=jpeg)
$y$和$x_2$的关系大概如图2所示。
![图1](https://mmbiz.qpic.cn/mmbiz_jpg/5Wj1VCYtIxOd9ozic3q56ClpNPFBIQiaRjichepbkSWFPuPnkTgszyrUlyCEjQlpZWHVKE6QPm5Vbs3K5HnKJv6Jg/0?wx_fmt=jpeg)
这101个数据是我按照$y=11.11+1.58x_1+0x_2+\varepsilon$构造出来的，$x_2 \sim Possion(10)$，$\varepsilon  \sim N(0, 1)$，因此，我们的预期是可以通过模型学习到房价大概和大小有个1.58倍的关系，和楼层没有关系，最后需要再加上11.11的固定的费用。
虽然小明并不知道我是怎么构造数据的，但他的直觉很准。他感觉可以用$f(x|\omega)=\omega_0+\omega_1x_1+\omega_2x_2$模拟房价与大小和楼层的关系，这就是一个线性回归模型。假设我们已经知道了$\omega$的值，那么只要小明知道某个房子的大小和楼层，就可以根据这个式子计算/预测出其房价。因此我们有房价的预测值$\hat{y}:=f(x|\omega)$。然而，$\omega$是未知的，是线性回归中待求解的未知参数。因此，我们需要找到一个合理的假设，比如“房价的预测值和真实值最接近”，在此假设下，我们可以写出一种模型的损失函数。对“接近”的不同定义方式，会导致不同的损失函数。一般而言，接近定义为误差的均方和最小，即损失函数$$J(\omega|x):=\frac{1}{2*101}\sum_{i=1}^{101}|y^i-\hat{y}^i|^2=\frac{1}{202}\sum_{i=1}^{101}|y^i-f(x^i|\omega)|^2=\frac{1}{202}\sum_{i=1}^{101}|y^i-\omega_0+\omega_1x^i_1+\omega_2x^i_2|^2$$
需要指出的是，回归模型$f(x|\omega)$是关自变量$x$的函数，损失函数$J(\omega|x)$是关于未知参数$\omega$的函数。上式中出现了数字101是因为我们共有101个已知数据。

>也可以用其他方式定义接近，如残差的绝对值的和最小；但是残差的和最小不是一个好的定义，因为残差有正有负，他们加在一起会相互抵消。


接下来需要求解模型，找到一个合适的$\omega$，使得损失函数最小。这是一个数学上的优化问题，最常用的方法是梯度下降法。梯度可以理解为多元函数的导数，是一个向量。在我们这个例子中，损失函数是一个关于$\omega$三元函数，那么它的梯度$\nabla{J(\omega)}$(我省略了$x$,因为和它没关系)就是一个三维向量，其中$\nabla_i{J(\omega)}$对应$J(\omega)$对于$\omega_i$的偏导数。对于任意一个多元函数，有一个结论是这样的，负梯度方向是函数值减少最快的方向。那么如果自变量沿着这个方向前行一小步，预期函数值是减小的。直观地讲，如果把函数值当做一座山的高度，那么负梯度方向好比我们下山的方向，沿着这个方向行走，海拔会降低。
![](https://mmbiz.qpic.cn/mmbiz_jpg/5Wj1VCYtIxMwGyvTa0N6ybC2Q52mfic6xtmdbBcy5891CyLZXD5UTZxGiaibUpMMOsib7GjutDsehAU8BWsFiaWhEjA/0?wx_fmt=jpeg)
因此，梯度下降法可以总结如下。首先为未知参数初始化一个值，然后在负梯度方向上搜索，即用$\omega:=\omega-\alpha\nabla{J(\omega)}$更新$\omega$，如此迭代，直到梯度为零或者$\omega$不变为止。

使用python，我们可以很容易地调用tensorflow(以下简称tf)内置的梯度下降优化器，只需要使用者有一些矩阵运算基础即可理解。tf给出的结果是$y=1.14+1.64x_1+0.34x_2$, 在极小值点$\omega^\star=(1.14,1.64,0.34)^T$,损失函数的极小值$J(\omega^\star)=3.385$。这是一个很不错的结果。接下来，我会在下一节介绍一些关于线性回归的零散的思考。

课程的最后一部分是关于矩阵运算的，初中算术的升级版，有兴趣的自己看就好了。

### 零散的糖
#### 从统计的角度看线性回归

我在上文中提到过，数据是按照$y=11.11+1.58x_1+0x_2+\varepsilon$构造的，最后的$\varepsilon \sim N(0, 1)$可以看成是随机的白噪声。或者可以这么解释，就是因变量$y$的的确确和其他因素有确定的线性关系，现在采集的数据并不能使这种关系严格成立，是因为采集数据时有些不可观测的误差$\varepsilon$。我们可以假设这些误差服从某个特定分布，继而可以构建一个模型。当假设服从正态分布时，就可以得到上文的线性回归模型。简要推导如下
标准正态分布的密度函数$$f(t)=\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{t^2}{2}}$$
那么观测到每个误差$\varepsilon_i$的概率$p_i=\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{\varepsilon_i^2}{2}}$，根据极大似然的思想，我们只要极大化$\prod{p_i}$。顺便说一下，个人感觉极大似然是个挺靠谱的方法，因为没有人可以说服我(一系列iid的)概率小的事件发生了，概率大的反而没发生，这不科学。然后把$\varepsilon_i=y^i-\omega_0+\omega_1x^i_1+\omega_2x^i_2$带入，并注意到极大化$\prod{p_i}$和极小化$-ln({\prod{p_i}})$是一回事，推导推导化简化简可以得出极大似然的目标是要极小化
$$\sum_{i}|y^i-\omega_0+\omega_1x^i_1+\omega_2x^i_2|^2$$
当然，如果假设误差服从其他的分布，可以得到不同的模型，广义线性回归说的就是这么个情况，我大概会在之后的文章说明，从广义上说，逻辑回归也是线性回归的一种。
####评价线性回归结果
我们有一系列观测数据$y_i$，那么可以计算它的“方差”$SST=\sum(y_i-\bar{y})^2$, 还可以计算回归模型预测值的方差$SSE=\sum(f_i-\bar{y})^2$，SSE和SST的比值可以认为观测数据的误差可以用回归模型解释的部分，称之为R方[^1]。一般而言，R方越大，可以认为模型越好（这里需要注意，要结合残差图判断，残差图如果明显是bias的，那就需要具体讨论了）。线性回归模型是有解析解的，我们上面用tf求出的解并不是最优的，最优解见下表。

|使用自变量个数|最优模型|损失函数值|R方|
|---|---|---|---|
|2 (tf求解)|$y=1.14+1.64x_1+0.34x_2$|3.3912|不存在因为没到最优解|
|2 (解析解)|$y=12.41+1.58x_1+0.13x_2$|0.3765|0.99964|
|1 (解析解)|$y=11.17+1.58x_1$|0.4688|0.99958|

这个表说明了两个问题，一个是同样是使用两个自变量，tf为什么求不出最优解；另一个是我们发现仅使用一个变量的模型(模型1)更加符合我构造数据的方式，为什么使用两个变量的模型(模型2)从R方和损失函数值上而言却都效果刚好。第二个问题其实是过拟合的问题。
#### 过拟合
过拟合的问题相对比较容易说明白，模型2的效果比模型1的效果好是在这次的101个特定数据上而言的。可以这么理解，有一批数据，我们只用一个维度得到一个模型，效果不错；那么再把第二个维度加进来考虑，有了更多的信息，效果肯定不会更差，因为只要令第二个维度的系数等于零就退化到了第一个模型。但是模型1确实更符合我构造数据的方式，这么看模型1更好。这是说，如果我按照相同的方式再构造一批新数据，在新数据上模型1的损失函数值反而更小，这个好才是我们想追求的好。这也说明了，机器学习为什么要把数据分为训练集和测试集。
过拟合还可以从另一方面看待。假设我们只有一个自变量，那么最多拟合线性模型$y=\omega_0+\omega_1x_1$，也可以拟合线性模型$y=\omega_0+\omega_1x_1+\omega_2x_1^{2}...+\omega_{100}x_1^{100}$。是的，虽然第二个式子出现了$x_1$的高次方项，它依然是个线性模型。可以指出一个平凡的事实，第二模型可以使得损失函数值等于零，和两点确定一条直线一样的道理，101个点也能确定一个100次方的式子。但是这个最小（0）的损失函数确有着很差的效果，因为它每一项的系数的绝对值都很大，看起来是一条震荡特别大的曲线，因此它在未知的测试集上的效果极差。这也是为什么机器学习算法会对未知参数正则化的原因。

#### tf求解
线性回归的损失函数是一个凸函数，那么使用梯度下降是一定可以求解到最优解的，但是我使用tf并没有得到这个解。最初我把学习率设为0.001，tf求解失败，给出的结果是NAN。之后把学习率设为0.0001才可以跑出一个结果。这其实不怪tf，而是我构造的那个合情合理的例子，在数据上而言是病态的。如果对原模型不进行任何变形，那么这个线性回归的design matrix [^2]虽然是凸的，但是它的三个特征向量分别是[1.10588680e+06 4.77482452e+00 1.76942078e+03]。也就是说这个凸函数在某个方向上变化特别快，而在另一个方向上变化相当缓慢。这会导致著名的zig zag效应，在使劲加迭代次数，tf也很难跑出更好的结果。
解决这个问题的办法就是对数据进行归一化，把数据的范围都约定都某个特定区间内，那么design matrix的每个特征值的量级大概都差不多，求解就要友好很多。数据归一化之后，学习率设为0.1，tf很快就给出了最优的损失函数值0.3766。

本文使用了mathematica和python，所有数据和代码在[这里]()可以找到。

### 参考文献

[^1]: [https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)

[^2]: [https://en.wikipedia.org/wiki/Design_matrix](https://en.wikipedia.org/wiki/Design_matrix)









